{"nbformat":4,"nbformat_minor":4,"metadata":{"language_info":{"file_extension":".py","version":"3.7.7","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"notebookId":"6c5fcbee-5f05-4c27-a4e2-cc5ce3bc32fb"},"cells":[{"cell_type":"markdown","source":"# Train model notebook","metadata":{"cellId":"9a93cec2-bcc9-471b-9884-9272d11983be"}},{"cell_type":"markdown","source":"## Import libraries and setup","metadata":{"cellId":"f0884825-6032-4fdc-899a-307ff50b0448"}},{"cell_type":"code","source":"DEBUG = True\nKAGGLE = False\nYDSP = True","metadata":{"cellId":"41b1f0cd-6411-4766-8bf7-663b2c547bb4"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if YDSP:\n    %pip install -U transformers\nelse:\n    !pwd","metadata":{"cellId":"r94tz24l828dsi7cvvfd0b"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#!g1.1\nimport warnings\nif DEBUG:\n    warnings.filterwarnings('ignore', category=UserWarning)\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\nfrom torch.utils.data.distributed import DistributedSampler\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\nos.environ['CUDA_VISIBLE_DEVICES'] = '0' if KAGGLE else '1'","metadata":{"cellId":"e14fd8fb-6b9d-4e87-95fe-50f919d5e9ce"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"VER = 'vdsp0'\nDATA_PATH = './data'\nMDLS_PATH = f'./models_{VER}'\nROBERTA_TYPE = 'deepset/xlm-roberta-large-squad2' # 'deepset/xlm-roberta-base-squad2'\nCONFIG = {\n    'folds': 5,\n    'fold_train': None, # 'None' or '0', '1', ..., '4'\n    'model_type': 'xlm_roberta',\n    'model_name_or_path':ROBERTA_TYPE, \n    'config_name': ROBERTA_TYPE,\n    'apex': False,\n    'grad_accum_steps': 1,\n    'tokenizer_name': ROBERTA_TYPE,\n    'max_seq_length': 384, # 256 or 384\n    'doc_stride': 128, \n    'epochs': 100,\n    'max_patience': 2,\n    'train_batch_size': 8,\n    'eval_batch_size': 16,\n    'optimizer_type': 'AdamW',\n    'learning_rate': 1.5e-5,\n    'weight_decay': 1e-2,\n    'epsilon': 1e-8,\n    'max_grad_norm': 1, # '1' or 'None'\n    'decay_name': 'linear-warmup',\n    'optimizer_grouped_parameters': False,\n    'warmup_ratio': .05,\n    'logging_steps': 500,\n    'output_dir': MDLS_PATH,\n    'seed': 2021\n}\nif not os.path.exists(MDLS_PATH):\n    os.mkdir(MDLS_PATH)\nwith open(f'{MDLS_PATH}/base_config.json', 'w') as file:\n    json.dump(CONFIG, file)\n        \ndef seed_all(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    print('optimal number of workers is', optimal_value)\n    return optimal_value\n\nstart_time = time.time()","metadata":{"cellId":"dd6868c2-4489-4f0f-ab19-ca6efd784a23"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Load and preprocess data","metadata":{"cellId":"e70a0542-ea4c-4a1e-8f57-385ae90247f2"}},{"cell_type":"code","source":"train = pd.read_csv(f'{DATA_PATH}/train.csv')\ntest = pd.read_csv(f'{DATA_PATH}/test.csv')\nexternal_mlqa = pd.read_csv(f'{DATA_PATH}/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv(f'{DATA_PATH}/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])\n\ndef create_folds(data, num_splits):\n    data['kfold'] = -1\n    kf = model_selection.StratifiedKFold(\n        n_splits=num_splits, \n        shuffle=True, \n        random_state=CONFIG['seed']\n    )\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n        data.loc[v_, 'kfold'] = f\n    return data\n\ntrain = create_folds(train, num_splits=CONFIG['folds'])\nexternal_train['kfold'] = -1\nexternal_train['id'] = list(np.arange(1, len(external_train) + 1))\ntrain = pd.concat([train, external_train]).reset_index(drop=True)\n\ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}\n\ntrain['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)","metadata":{"cellId":"fad38886-9e33-4cfd-a603-08b352e647dc"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#!g1.1\ndef prepare_train_features(config, example, tokenizer):\n    example['question'] = example['question'].lstrip()\n    tokenized_example = tokenizer(\n        example['question'],\n        example['context'],\n        truncation='only_second',\n        max_length=config['max_seq_length'],\n        stride=config['doc_stride'],\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding='max_length'\n    )\n    sample_mapping = tokenized_example.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_example.pop('offset_mapping')\n    features = []\n    for i, offsets in enumerate(offset_mapping):\n        feature = {}\n        input_ids = tokenized_example['input_ids'][i]\n        attention_mask = tokenized_example['attention_mask'][i]\n        feature['input_ids'] = input_ids\n        feature['attention_mask'] = attention_mask\n        feature['offset_mapping'] = offsets\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_example.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = example['answers']\n        if len(answers['answer_start']) == 0:\n            feature['start_position'] = cls_index\n            feature['end_position'] = cls_index\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                feature['start_position'] = cls_index\n                feature['end_position'] = cls_index\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                feature['start_position'] = token_start_index - 1\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                feature['end_position'] = token_end_index + 1\n        features.append(feature)\n    return features","metadata":{"cellId":"f4f7585e-fdc2-456e-ac21-b6a895390a08"},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Dataset retriever","metadata":{"cellId":"35d25ee2-34a6-4bb0-bd0b-5bfcd25b6825"}},{"cell_type":"code","source":"#!g1.1\nclass DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","metadata":{"cellId":"cc6cb8fc-b1f2-4c29-9e87-17fc41a606fb"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Build a model","metadata":{"cellId":"217f0a9a-93fa-41f0-a6f2-182ab74cbdfb"}},{"cell_type":"code","source":"#!g1.1\nclass Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(\n            modelname_or_path, \n            config=config\n        )\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(\n                mean=0, \n                std=self.config.initializer_range\n            )\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n        # token_type_ids=None\n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        return start_logits, end_logits","metadata":{"cellId":"6df019c2-d3d6-4374-836a-8b4743bb7752"},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Loss","metadata":{"cellId":"14743075-a689-4d53-9abf-239a172be433"}},{"cell_type":"code","source":"#!g1.1\ndef loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(\n        start_preds,\n        start_labels\n    )\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(\n        end_preds, \n        end_labels\n    )\n    total_loss = (start_loss + end_loss) / 2\n    return total_loss","metadata":{"cellId":"5bdd2c41-e8b3-440b-954e-ff7eb4ecaa0b"},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Grouped Layerwise Learning Rate Decay","metadata":{"cellId":"61c970f5-be7e-4508-b871-f19798321912"}},{"cell_type":"code","source":"#!g1.1\ndef get_optimizer_grouped_parameters(config, model):\n    no_decay = ['bias', 'LayerNorm.weight']\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.',\n               'layer.4.','layer.5.','layer.6.','layer.7.',\n               'layer.8.','layer.9.','layer.10.','layer.11.']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if not any(nd in n for nd in no_decay) \n                    and not any(nd in n for nd in group_all)],\n         'weight_decay': config['weight_decay']},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if not any(nd in n for nd in no_decay) \n                    and any(nd in n for nd in group1)],\n         'weight_decay': config['weight_decay'], \n         'lr': config['learning_rate'] / 2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if not any(nd in n for nd in no_decay) \n                    and any(nd in n for nd in group2)],\n         'weight_decay': config['weight_decay'], \n         'lr': config['learning_rate']},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if not any(nd in n for nd in no_decay) \n                    and any(nd in n for nd in group3)],\n         'weight_decay': config['weight_decay'], \n         'lr': config['learning_rate'] * 2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if any(nd in n for nd in no_decay) \n                    and not any(nd in n for nd in group_all)],\n         'weight_decay': 0},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if any(nd in n for nd in no_decay) \n                    and any(nd in n for nd in group1)],\n         'weight_decay': 0, \n         'lr': config['learning_rate'] / 2.6},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if any(nd in n for nd in no_decay) \n                    and any(nd in n for nd in group2)],\n         'weight_decay': 0, 'lr': config['learning_rate']},\n        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n                    if any(nd in n for nd in no_decay) \n                    and any(nd in n for nd in group3)],\n         'weight_decay': 0, \n         'lr': config['learning_rate'] * 2.6},\n        {'params': [p for n, p in model.named_parameters() \n                    if config['model_type'] not in n], \n         'weight_decay': 0,\n         'lr': config['learning_rate'] * 20},\n    ]\n    return optimizer_grouped_parameters","metadata":{"cellId":"f946f7cf-c9ef-4729-b9d1-9a22af2190f5"},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Metric logger","metadata":{"cellId":"f883745f-84e7-4f90-8a07-8de7416bfcba"}},{"cell_type":"code","source":"#!g1.1\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","metadata":{"cellId":"5f21b785-b60b-4afe-b0b4-eef4eba6dcf6"},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Utilities","metadata":{"cellId":"b94444b4-e8fd-4bb8-8134-c2ce48ca7c75"}},{"cell_type":"code","source":"#!g1.1\ndef make_model(config):\n    model_config = AutoConfig.from_pretrained(config['config_name'])\n    model = Model(config['model_name_or_path'], config=model_config)\n    return model_config, model\n\ndef make_optimizer(config, model):\n    if config['optimizer_grouped_parameters']:\n        optimizer_grouped_parameters = get_optimizer_grouped_parameters(config, model)\n    else:\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {\n                'params': [p for n, p in model.named_parameters() \n                           if not any(nd in n for nd in no_decay)],\n                'weight_decay': config['weight_decay'],\n            },\n            {\n                'params': [p for n, p in model.named_parameters() \n                           if any(nd in n for nd in no_decay)],\n                'weight_decay': 0,\n            },\n        ]\n    if config['optimizer_type'] == 'AdamW':\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=config['learning_rate'],\n            eps=config['epsilon'],\n            correct_bias=True\n        )\n    return optimizer\n\ndef make_scheduler(\n    config, optimizer, \n    num_warmup_steps, \n    num_training_steps\n):\n    if config['decay_name'] == 'cosine-warmup':\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    return scheduler    \n\ndef make_loader(\n    config, data, \n    tokenizer, fold\n):\n    train_set, val_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n    train_features, val_features = [[] for _ in range(2)]\n    for i, row in train_set.iterrows():\n        train_features += prepare_train_features(config, row, tokenizer)\n    for i, row in val_set.iterrows():\n        val_features += prepare_train_features(config, row, tokenizer)\n    train_dataset = DatasetRetriever(train_features)\n    val_dataset = DatasetRetriever(val_features)\n    print(f'num examples train: {len(train_dataset)}', \n          f'num examples val: {len(val_dataset)}')\n    train_sampler = RandomSampler(train_dataset)\n    val_sampler = SequentialSampler(val_dataset)\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=config['train_batch_size'],\n        sampler=train_sampler,\n        num_workers=optimal_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=config['eval_batch_size'], \n        sampler=val_sampler,\n        num_workers=optimal_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n    return train_dataloader, val_dataloader","metadata":{"cellId":"fff1167e-7bc1-4ef6-8924-7eec49e7c41e"},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Trainer and evaluator","metadata":{"cellId":"513e9de5-a4a4-4407-9fc2-14c326bbd7cb"}},{"cell_type":"code","source":"#!g1.1\nclass Trainer:\n    def __init__(\n        self, model, tokenizer, \n        optimizer, scheduler\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def train(\n        self, config, \n        train_dataloader, \n        epoch, result_dict\n    ):\n        print('=' * 10, f'train {epoch} epoch', '=' * 10)\n        count = 0\n        losses = AverageMeter()\n        self.model.zero_grad()\n        self.model.train()\n        if config['seed']:\n            seed_all(config['seed'])\n        if config['apex']:\n            scaler = torch.cuda.amp.GradScaler()\n        for batch_idx, batch_data in enumerate(train_dataloader):\n            if config['apex']:\n                with torch.cuda.amp.autocast():\n                    input_ids, attention_mask, targets_start, targets_end = \\\n                        batch_data['input_ids'], batch_data['attention_mask'], \\\n                            batch_data['start_position'], batch_data['end_position']\n                    input_ids, attention_mask, targets_start, targets_end = \\\n                        input_ids.cuda(), attention_mask.cuda(), \\\n                            targets_start.cuda(), targets_end.cuda()\n                    outputs_start, outputs_end = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                    )\n                    loss = loss_fn((outputs_start, outputs_end), \n                                   (targets_start, targets_end))\n                    loss = loss / config['grad_accum_steps']\n                    scaler.scale(loss).backward()\n                    if config['max_grad_norm']:\n                        torch.nn.utils.clip_grad_norm_(\n                            self.model.parameters(), \n                            config['max_grad_norm']\n                        )\n                    if batch_idx % config['grad_accum_steps'] == 0 or batch_idx == len(train_dataloader) - 1:\n                        scaler.step(self.optimizer)\n                        scaler.update()\n                        self.optimizer.zero_grad()\n            else:\n                input_ids, attention_mask, targets_start, targets_end = \\\n                    batch_data['input_ids'], batch_data['attention_mask'], \\\n                        batch_data['start_position'], batch_data['end_position']\n                input_ids, attention_mask, targets_start, targets_end = \\\n                    input_ids.cuda(), attention_mask.cuda(), \\\n                        targets_start.cuda(), targets_end.cuda()\n                outputs_start, outputs_end = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                )\n                loss = loss_fn((outputs_start, outputs_end), \n                               (targets_start, targets_end))\n                loss = loss / config['grad_accum_steps']\n                loss.backward()\n                if config['max_grad_norm']:\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), \n                        config['max_grad_norm']\n                    )\n                if batch_idx % config['grad_accum_steps'] == 0 or batch_idx == len(train_dataloader) - 1:\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n            self.scheduler.step()\n            count += input_ids.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n            if (batch_idx % config['logging_steps'] == 0) or (batch_idx + 1) == len(train_dataloader):\n                _s = str(len(str(len(train_dataloader.sampler))))\n                ret = [\n                    ('epoch: {: >2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(\n                        epoch, \n                        count, \n                        len(train_dataloader.sampler), \n                        100 * count / len(train_dataloader.sampler)\n                    ),\n                    'train loss: {: >4.5f}'.format(losses.avg),\n                ]\n                end_ = '\\n' if YDSP else '\\r'\n                print(', '.join(ret), end=end_)\n        print()\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","metadata":{"cellId":"22a6548f-f123-4dc2-9366-c047353fd28a"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#!g1.1\nclass Evaluator:\n    def __init__(self, model):\n        self.model = model\n    \n    def save(self, result, output_dir):\n        with open(f'{output_dir}/result_dict.json', 'w') as f:\n            f.write(json.dumps(\n                result, \n                sort_keys=True, \n                indent=4, \n                ensure_ascii=False))\n\n    def evaluate(self, config, \n                 val_dataloader, \n                 epoch, result_dict):\n        count = 0\n        losses = AverageMeter()\n        for batch_idx, batch_data in enumerate(val_dataloader):\n            self.model = self.model.eval()\n            input_ids, attention_mask, targets_start, targets_end = \\\n                batch_data['input_ids'], batch_data['attention_mask'], \\\n                    batch_data['start_position'], batch_data['end_position']\n            input_ids, attention_mask, targets_start, targets_end = \\\n                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), \\\n                    targets_end.cuda()\n            with torch.no_grad():            \n                outputs_start, outputs_end = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                )\n                loss = loss_fn((outputs_start, outputs_end), \n                               (targets_start, targets_end))\n                count += input_ids.size(0)\n                losses.update(loss.item(), input_ids.size(0))\n            if (batch_idx % config['logging_steps'] == 0) or (batch_idx + 1) == len(val_dataloader):\n                _s = str(len(str(len(val_dataloader.sampler))))\n                ret = [\n                    ('epoch: {: >2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(\n                        epoch, \n                        count, \n                        len(val_dataloader.sampler), \n                        100 * count / len(val_dataloader.sampler)\n                    ),\n                    'val loss: {: >4.5f}'.format(losses.avg),\n                ]\n                end_ = '\\n' if YDSP else '\\r'\n                print(', '.join(ret), end=end_)\n        print()\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict","metadata":{"cellId":"3be6a38d-465b-498a-8701-e5dd1716a68e"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Run engine","metadata":{"cellId":"23c0bb80-584f-45d7-94fc-31146df69655"}},{"cell_type":"code","source":"#!g1.1\ndef init_training(config, data, tokenizer, fold):\n    if config['seed']:\n        seed_all(config['seed'])\n    if not os.path.exists(config['output_dir']):\n        os.makedirs(config['output_dir'])\n    model_config, model = make_model(config)\n    if torch.cuda.device_count() >= 1:\n        print('model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n    train_dataloader, val_dataloader = make_loader(config, data, tokenizer, fold)\n    optimizer = make_optimizer(config, model)\n    num_training_steps = math.ceil(\n        len(train_dataloader) / config['grad_accum_steps']\n    ) * config['epochs']\n    if config['warmup_ratio'] > 0:\n        num_warmup_steps = int(config['warmup_ratio'] * num_training_steps)\n    else:\n        num_warmup_steps = 0\n    print(f'total train steps: {num_training_steps}',\n          f'| total warmup steps: {num_warmup_steps}')\n    scheduler = make_scheduler(\n        config, \n        optimizer, \n        num_warmup_steps, \n        num_training_steps\n    )\n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'best_val_loss': np.inf\n    }\n    return (model, model_config, optimizer, \n            scheduler, train_dataloader, \n            val_dataloader, result_dict)","metadata":{"cellId":"c0e9d6df-b2f6-48da-a113-967ead34e9f4"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#!g1.1\ndef run(data, tokenizer, fold, epochs, max_patience):\n    model, model_config, \\\n        optimizer, scheduler, train_dataloader, \\\n            val_dataloader, result_dict = init_training(CONFIG, data, \n                                                        tokenizer, fold)\n    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n    evaluator = Evaluator(model)\n    train_time_list = []\n    val_time_list = []\n    n_patience = 0\n    for epoch in range(epochs):\n        result_dict['epoch'].append(epoch)\n        torch.cuda.synchronize()\n        ep_time = time.time()\n        result_dict = trainer.train(\n            CONFIG, train_dataloader, \n            epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        train_time_list.append(time.time() - ep_time)\n        torch.cuda.synchronize()\n        ep_time = time.time()\n        result_dict = evaluator.evaluate(\n            CONFIG, val_dataloader, \n            epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        val_time_list.append(time.time() - ep_time)\n        output_dir = os.path.join(\n            CONFIG['output_dir'], \n            f'checkpoint-fold-{fold}'\n        )\n        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n            print('{} epoch -> best epoch updated, val loss: {: >4.5f}'.format(\n                epoch, \n                result_dict['val_loss'][-1]\n            ))\n            result_dict['best_val_loss'] = result_dict['val_loss'][-1]        \n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(model.state_dict(), f'{output_dir}/pytorch_model.bin')\n            model_config.save_pretrained(output_dir)\n            model_config.save_pretrained(MDLS_PATH)\n            print(f'saving model checkpoint to {output_dir}')\n            n_patience = 0\n        else:\n            n_patience += 1\n        if n_patience >= max_patience:\n            print(f'no val loss improvement for last {n_patience} epochs')\n            break\n    evaluator.save(result_dict, output_dir)\n    print(\n        f'total train time: {np.sum(train_time_list) // 60:.0f} min',\n        f'{np.sum(train_time_list) % 60:.0f} secs | ', \n        f'average per epoch: {np.mean(train_time_list) // 60:.0f} min',\n        f'{np.mean(train_time_list)  % 60:.0f} secs'\n    )\n    print(\n        f'total val time: {np.sum(val_time_list) // 60:.0f} min',\n        f'{np.sum(val_time_list) % 60:.0f} secs | ', \n        f'average per epoch: {np.mean(val_time_list) // 60:.0f} min',\n        f'{np.mean(val_time_list)  % 60:.0f} secs'\n    )\n    del trainer, evaluator, model, model_config, tokenizer, \\\n        optimizer, scheduler, train_dataloader, val_dataloader, result_dict\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"cellId":"640f4383-48e1-4a8f-9de6-d17d89380341"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#!g1.1\nTOKENIZER = AutoTokenizer.from_pretrained(CONFIG['tokenizer_name'])\nTOKENIZER.save_pretrained(CONFIG['output_dir'])\n\nif CONFIG['fold_train']:\n    start_fold = CONFIG['fold_train'] \n    end_fold = start_fold + 1\nelse:\n    start_fold = 0 \n    end_fold = CONFIG['folds']\nfor fold in range(start_fold, end_fold):\n    content = ' '.join(['=' * 20, f'FOLD: {fold}', '=' * 20])\n    print('=' * len(content))\n    print(content)\n    print('=' * len(content))\n    run(train, TOKENIZER, fold, CONFIG['epochs'], CONFIG['max_patience'])\n\nelapsed_time = time.time() - start_time\nprint(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')","metadata":{"cellId":"42a4ab7a-ed8e-4e8f-8771-f5c453288460"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"bb94c02d-fd91-4226-a07d-2af1e73401e4"},"outputs":[],"execution_count":18}]}