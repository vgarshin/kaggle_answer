{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "df95dfb7-e323-4f74-a8e5-3f40546aa70f"
   },
   "source": [
    "# Train model notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "a566a06c-5e81-4fbe-9591-dba8dda37c99"
   },
   "source": [
    "## Import libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "738dcec1-c79c-4592-bfc6-e87b1ec6f9b9"
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "YSDP = False\n",
    "KAGGLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "467ee586-9941-4b5e-be4d-0e8b8ceab63b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "if DEBUG:\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' if KAGGLE else '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5cbc40d3-0fd3-44ee-847d-7bdfc72be350"
   },
   "outputs": [],
   "source": [
    "VER = 'v6'\n",
    "DATA_PATH = './data'\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "ROBERTA_TYPE = 'deepset/xlm-roberta-base-squad2' # 'deepset/xlm-roberta-base-squad2'\n",
    "CONFIG = {\n",
    "    'ver': VER,\n",
    "    'folds': 5,\n",
    "    'fold_train': None, # 'None' or '0', '1', ..., '4'\n",
    "    'model_type': 'xlm_roberta',\n",
    "    'model_name_or_path':ROBERTA_TYPE,\n",
    "    'config_name': ROBERTA_TYPE,\n",
    "    'apex': False,\n",
    "    'grad_accum_steps': 2,\n",
    "    'tokenizer_name': ROBERTA_TYPE,\n",
    "    'max_seq_length': 196, # 256 or 384\n",
    "    'doc_stride': 64, \n",
    "    'epochs': 100,\n",
    "    'max_patience': 2,\n",
    "    'train_batch_size': 4,\n",
    "    'eval_batch_size': 8,\n",
    "    'optimizer_type': 'AdamW',\n",
    "    'learning_rate': 1.5e-5,\n",
    "    'weight_decay': 1e-2,\n",
    "    'epsilon': 1e-8,\n",
    "    'max_grad_norm': 1, # '1' or 'None'\n",
    "    'decay_name': 'linear-warmup',\n",
    "    'optimizer_grouped_parameters': False,\n",
    "    'warmup_ratio': .1,\n",
    "    'logging_steps': 500,\n",
    "    'output_dir': MDLS_PATH,\n",
    "    'seed': None\n",
    "}\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/base_config.json', 'w') as file:\n",
    "    json.dump(CONFIG, file)\n",
    "        \n",
    "def seed_all(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    print('optimal number of workers is', optimal_value)\n",
    "    return optimal_value\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7ff895a6-a5e4-41c5-89f0-71a7bf7d0887"
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "840ffdb2-b7ca-49a8-9a6b-10814e462268"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
    "test = pd.read_csv(f'{DATA_PATH}/test.csv')\n",
    "external_mlqa = pd.read_csv(f'{DATA_PATH}/mlqa_hindi.csv')\n",
    "external_xquad = pd.read_csv(f'{DATA_PATH}/xquad.csv')\n",
    "external_train = pd.concat([external_mlqa, external_xquad])\n",
    "\n",
    "def create_folds(data, num_splits):\n",
    "    data['kfold'] = -1\n",
    "    kf = model_selection.StratifiedKFold(\n",
    "        n_splits=num_splits, \n",
    "        shuffle=True, \n",
    "        random_state=CONFIG['seed']\n",
    "    )\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, num_splits=CONFIG['folds'])\n",
    "external_train['kfold'] = -1\n",
    "external_train['id'] = list(np.arange(1, len(external_train) + 1))\n",
    "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
    "\n",
    "def convert_answers(row):\n",
    "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
    "\n",
    "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "7bd6db53-204e-4c10-80e4-bb19a12bd0e8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def prepare_train_features(config, example, tokenizer):\n",
    "    example['question'] = example['question'].lstrip()\n",
    "    tokenized_example = tokenizer(\n",
    "        example['question'],\n",
    "        example['context'],\n",
    "        truncation='only_second',\n",
    "        max_length=config['max_seq_length'],\n",
    "        stride=config['doc_stride'],\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    sample_mapping = tokenized_example.pop('overflow_to_sample_mapping')\n",
    "    offset_mapping = tokenized_example.pop('offset_mapping')\n",
    "    features = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        feature = {}\n",
    "        input_ids = tokenized_example['input_ids'][i]\n",
    "        attention_mask = tokenized_example['attention_mask'][i]\n",
    "        feature['input_ids'] = input_ids\n",
    "        feature['attention_mask'] = attention_mask\n",
    "        feature['offset_mapping'] = offsets\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_example.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example['answers']\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            feature['start_position'] = cls_index\n",
    "            feature['end_position'] = cls_index\n",
    "        else:\n",
    "            start_char = answers['answer_start'][0]\n",
    "            end_char = start_char + len(answers['text'][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                feature['start_position'] = cls_index\n",
    "                feature['end_position'] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                feature['start_position'] = token_start_index - 1\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                feature['end_position'] = token_end_index + 1\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8235a9db-cf0d-4ae9-969b-dadcbb5f0e72"
   },
   "source": [
    "## Dataset retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "c5df0c4d-29d3-43e7-885c-09ae75d93d72"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':feature['offset_mapping'],\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                'question': feature['question']\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "c11618cb-8c36-4dfc-9092-ed79c4464207"
   },
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9bee47de-d946-4eff-b41c-96e6fbcea2db"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(\n",
    "            modelname_or_path, \n",
    "            config=config\n",
    "        )\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self._init_weights(self.qa_outputs)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(\n",
    "                mean=0, \n",
    "                std=self.config.initializer_range\n",
    "            )\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        # token_type_ids=None\n",
    "    ):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        # sequence_output = self.dropout(sequence_output)\n",
    "        qa_logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "a65ee700-e056-44e8-acbe-030602a7fa66"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b2abb310-cb78-4beb-ba65-46bb8f41469e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds = preds\n",
    "    start_labels, end_labels = labels\n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(\n",
    "        start_preds,\n",
    "        start_labels\n",
    "    )\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(\n",
    "        end_preds, \n",
    "        end_labels\n",
    "    )\n",
    "    total_loss = (start_loss + end_loss) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ec46a35e-62ae-4c00-b1a9-fc715f03e7df"
   },
   "source": [
    "## Grouped Layerwise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ed640504-092a-415b-9578-fc90466f1102"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def get_optimizer_grouped_parameters(config, model):\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.',\n",
    "               'layer.4.','layer.5.','layer.6.','layer.7.',\n",
    "               'layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay) \n",
    "                    and not any(nd in n for nd in group_all)],\n",
    "         'weight_decay': config['weight_decay']},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay) \n",
    "                    and any(nd in n for nd in group1)],\n",
    "         'weight_decay': config['weight_decay'], \n",
    "         'lr': config['learning_rate'] / 2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay) \n",
    "                    and any(nd in n for nd in group2)],\n",
    "         'weight_decay': config['weight_decay'], \n",
    "         'lr': config['learning_rate']},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if not any(nd in n for nd in no_decay) \n",
    "                    and any(nd in n for nd in group3)],\n",
    "         'weight_decay': config['weight_decay'], \n",
    "         'lr': config['learning_rate'] * 2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay) \n",
    "                    and not any(nd in n for nd in group_all)],\n",
    "         'weight_decay': 0},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay) \n",
    "                    and any(nd in n for nd in group1)],\n",
    "         'weight_decay': 0, \n",
    "         'lr': config['learning_rate'] / 2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay) \n",
    "                    and any(nd in n for nd in group2)],\n",
    "         'weight_decay': 0, 'lr': config['learning_rate']},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() \n",
    "                    if any(nd in n for nd in no_decay) \n",
    "                    and any(nd in n for nd in group3)],\n",
    "         'weight_decay': 0, \n",
    "         'lr': config['learning_rate'] * 2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() \n",
    "                    if config['model_type'] not in n], \n",
    "         'weight_decay': 0,\n",
    "         'lr': config['learning_rate'] * 20},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "a7bb8452-1841-47a3-ac75-f1cfaa8c2b31"
   },
   "source": [
    "## Metric logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "26a5d319-638c-4fc1-bc86-b328a5bbe9f4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "051a0945-169a-4f8a-89d6-8f8dd64d5388"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "61e605c9-46ce-4ccf-8c8d-52981f6f41c0"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def make_model(config):\n",
    "    model_config = AutoConfig.from_pretrained(config['config_name'])\n",
    "    model = Model(config['model_name_or_path'], config=model_config)\n",
    "    return model_config, model\n",
    "\n",
    "def make_optimizer(config, model):\n",
    "    if config['optimizer_grouped_parameters']:\n",
    "        optimizer_grouped_parameters = get_optimizer_grouped_parameters(config, model)\n",
    "    else:\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() \n",
    "                           if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': config['weight_decay'],\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() \n",
    "                           if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0,\n",
    "            },\n",
    "        ]\n",
    "    if config['optimizer_type'] == 'AdamW':\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=config['learning_rate'],\n",
    "            eps=config['epsilon'],\n",
    "            correct_bias=True\n",
    "        )\n",
    "    return optimizer\n",
    "\n",
    "def make_scheduler(\n",
    "    config, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if config['decay_name'] == 'cosine-warmup':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    config, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, val_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_features, val_features = [[] for _ in range(2)]\n",
    "    for i, row in train_set.iterrows():\n",
    "        train_features += prepare_train_features(config, row, tokenizer)\n",
    "    for i, row in val_set.iterrows():\n",
    "        val_features += prepare_train_features(config, row, tokenizer)\n",
    "    train_dataset = DatasetRetriever(train_features)\n",
    "    val_dataset = DatasetRetriever(val_features)\n",
    "    print(f'num examples train: {len(train_dataset)}', \n",
    "          f'num examples val: {len(val_dataset)}')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    val_sampler = SequentialSampler(val_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['train_batch_size'],\n",
    "        sampler=train_sampler,\n",
    "        num_workers=optimal_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['eval_batch_size'], \n",
    "        sampler=val_sampler,\n",
    "        num_workers=optimal_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3f9ae80c-7b17-4176-b6d8-1baeeea19a7e"
   },
   "source": [
    "## Trainer and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9ec71002-5705-40dd-b593-8c673f14d9d8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train(\n",
    "        self, config, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        print('=' * 10, f'train {epoch} epoch', '=' * 10)\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        if config['seed']:\n",
    "            seed_all(config['seed'])\n",
    "        if config['apex']:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            if config['apex']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                        batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                            batch_data['start_position'], batch_data['end_position']\n",
    "                    input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                        input_ids.cuda(), attention_mask.cuda(), \\\n",
    "                            targets_start.cuda(), targets_end.cuda()\n",
    "                    outputs_start, outputs_end = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    loss = loss_fn((outputs_start, outputs_end), \n",
    "                                   (targets_start, targets_end))\n",
    "                    loss = loss / config['grad_accum_steps']\n",
    "                    scaler.scale(loss).backward()\n",
    "                    if config['max_grad_norm']:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), \n",
    "                            config['max_grad_norm']\n",
    "                        )\n",
    "                    if batch_idx % config['grad_accum_steps'] == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                        scaler.step(self.optimizer)\n",
    "                        scaler.update()\n",
    "                        self.optimizer.zero_grad()\n",
    "            else:\n",
    "                input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                    batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                        batch_data['start_position'], batch_data['end_position']\n",
    "                input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                    input_ids.cuda(), attention_mask.cuda(), \\\n",
    "                        targets_start.cuda(), targets_end.cuda()\n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                loss = loss_fn((outputs_start, outputs_end), \n",
    "                               (targets_start, targets_end))\n",
    "                loss = loss / config['grad_accum_steps']\n",
    "                loss.backward()\n",
    "                if config['max_grad_norm']:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), \n",
    "                        config['max_grad_norm']\n",
    "                    )\n",
    "                if batch_idx % config['grad_accum_steps'] == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "            count += input_ids.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "            if (batch_idx % config['logging_steps'] == 0) or (batch_idx + 1) == len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('epoch: {: >2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(\n",
    "                        epoch, \n",
    "                        count, \n",
    "                        len(train_dataloader.sampler), \n",
    "                        100 * count / len(train_dataloader.sampler)\n",
    "                    ),\n",
    "                    'train loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret), end='\\n' if YSDP else '\\r')\n",
    "        print()\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0e07f1d7-d0cc-4a7e-aa2f-5879bd0cd810"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(\n",
    "                result, \n",
    "                sort_keys=True, \n",
    "                indent=4, \n",
    "                ensure_ascii=False))\n",
    "\n",
    "    def evaluate(\n",
    "        self, config,\n",
    "        val_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(val_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), \\\n",
    "                    targets_end.cuda()\n",
    "            with torch.no_grad():            \n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                loss = loss_fn((outputs_start, outputs_end), \n",
    "                               (targets_start, targets_end))\n",
    "                count += input_ids.size(0)\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "            if (batch_idx % config['logging_steps'] == 0) or (batch_idx + 1) == len(val_dataloader):\n",
    "                _s = str(len(str(len(val_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('epoch: {: >2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(\n",
    "                        epoch, \n",
    "                        count, \n",
    "                        len(val_dataloader.sampler), \n",
    "                        100 * count / len(val_dataloader.sampler)\n",
    "                    ),\n",
    "                    'val loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret), end='\\n' if YSDP else '\\r')\n",
    "        print()\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "779bbbf3-7248-4603-92a4-a0a8eb5a4ccf"
   },
   "source": [
    "## Run engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cdf5b40a-fcf1-4e7e-8605-2fb850a88a66"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def init_training(config, data, tokenizer, fold):\n",
    "    if config['seed']:\n",
    "        seed_all(config['seed'])\n",
    "    if not os.path.exists(config['output_dir']):\n",
    "        os.makedirs(config['output_dir'])\n",
    "    model_config, model = make_model(config)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    train_dataloader, val_dataloader = make_loader(config, data, tokenizer, fold)\n",
    "    optimizer = make_optimizer(config, model)\n",
    "    num_training_steps = math.ceil(\n",
    "        len(train_dataloader) / config['grad_accum_steps']\n",
    "    ) * config['epochs']\n",
    "    if config['warmup_ratio'] > 0:\n",
    "        num_warmup_steps = int(config['warmup_ratio'] * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f'total train steps: {num_training_steps}',\n",
    "          f'| total warmup steps: {num_warmup_steps}')\n",
    "    scheduler = make_scheduler(\n",
    "        config, \n",
    "        optimizer, \n",
    "        num_warmup_steps, \n",
    "        num_training_steps\n",
    "    )\n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "    return (model, model_config, optimizer, \n",
    "            scheduler, train_dataloader, \n",
    "            val_dataloader, result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1d001233-3355-4bdc-a487-70f895931c69"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def run(data, tokenizer, fold, epochs, max_patience):\n",
    "    model, model_config, \\\n",
    "        optimizer, scheduler, train_dataloader, \\\n",
    "            val_dataloader, result_dict = init_training(CONFIG, data, \n",
    "                                                        tokenizer, fold)\n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
    "    evaluator = Evaluator(model)\n",
    "    train_time_list = []\n",
    "    val_time_list = []\n",
    "    n_patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "        torch.cuda.synchronize()\n",
    "        ep_time = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            CONFIG, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        train_time_list.append(time.time() - ep_time)\n",
    "        torch.cuda.synchronize()\n",
    "        ep_time = time.time()\n",
    "        result_dict = evaluator.evaluate(\n",
    "            CONFIG, val_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        val_time_list.append(time.time() - ep_time)\n",
    "        output_dir = os.path.join(\n",
    "            CONFIG['output_dir'], \n",
    "            f'checkpoint-fold-{fold}'\n",
    "        )\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print('{} epoch -> best epoch updated, val loss: {: >4.5f}'.format(\n",
    "                epoch, \n",
    "                result_dict['val_loss'][-1]\n",
    "            ))\n",
    "            result_dict['best_val_loss'] = result_dict['val_loss'][-1]        \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{output_dir}/pytorch_model.bin')\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            model_config.save_pretrained(MDLS_PATH)\n",
    "            print(f'saving model checkpoint to {output_dir}')\n",
    "            n_patience = 0\n",
    "        else:\n",
    "            n_patience += 1\n",
    "        if n_patience >= max_patience:\n",
    "            print(f'no val loss improvement for last {n_patience} epochs')\n",
    "            break\n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    print(\n",
    "        f'total train time: {np.sum(train_time_list) // 60:.0f} min',\n",
    "        f'{np.sum(train_time_list) % 60:.0f} secs | ', \n",
    "        f'average per epoch: {np.mean(train_time_list) // 60:.0f} min',\n",
    "        f'{np.mean(train_time_list)  % 60:.0f} secs'\n",
    "    )\n",
    "    print(\n",
    "        f'total val time: {np.sum(val_time_list) // 60:.0f} min',\n",
    "        f'{np.sum(val_time_list) % 60:.0f} secs | ', \n",
    "        f'average per epoch: {np.mean(val_time_list) // 60:.0f} min',\n",
    "        f'{np.mean(val_time_list)  % 60:.0f} secs'\n",
    "    )\n",
    "    del trainer, evaluator, model, model_config, tokenizer, \\\n",
    "        optimizer, scheduler, train_dataloader, val_dataloader, result_dict\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b9d8c1a4-1a39-4bc4-b427-c5974c44cdd4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CONFIG['tokenizer_name'])\n",
    "TOKENIZER.save_pretrained(CONFIG['output_dir'])\n",
    "\n",
    "if CONFIG['fold_train']:\n",
    "    start_fold = CONFIG['fold_train'] \n",
    "    end_fold = start_fold + 1\n",
    "else:\n",
    "    start_fold = 0 \n",
    "    end_fold = CONFIG['folds']\n",
    "for fold in range(start_fold, end_fold):\n",
    "    content = ' '.join(['=' * 20, f'FOLD: {fold}', '=' * 20])\n",
    "    print('=' * len(content))\n",
    "    print(content)\n",
    "    print('=' * len(content))\n",
    "    run(train, TOKENIZER, fold, CONFIG['epochs'], CONFIG['max_patience'])\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9aa13866-1ff2-4ad9-96ce-1dd3232e7134"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "notebookId": "43d51f3c-868b-4b5c-86f0-7b96b23bf9cc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
